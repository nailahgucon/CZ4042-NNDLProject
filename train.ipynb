{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0effb481-0ec8-4898-a7fc-5144c7b128b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torchvision.transforms import CenterCrop, Resize, Compose, ToTensor, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44abd4a0-1c65-403f-a1a7-fe030b3b032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Compose([\n",
    "    Resize((480,480)),\n",
    "    CenterCrop(480),\n",
    "    Normalize(mean =[0.485, 0.456, 0.406], std =[0.229, 0.224, 0.225] )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a876ee41-e3b8-4c08-9265-f8be30bd09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        self.key_frame = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.to_list()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.key_frame.iloc[idx,0])\n",
    "        image = Image.open(img_name)\n",
    "        image = ToTensor()(image)\n",
    "        label = torch.tensor(self.key_frame.iloc[idx, 1])\n",
    "\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9744763a-4eb8-498f-923b-98b806bcb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame = pd.read_csv(\"labels_dino.csv\") #importing the csv file with the labels of the key frames\n",
    "train,test = train_test_split(key_frame, test_size = 0.2)  #splitting the data into train and test sets\n",
    "train = pd.DataFrame(train) \n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "batch_size = 4\n",
    "trainset = DinoDataset(root_dir = \"captures\", dataframe = train, transform = transformer)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size) \n",
    "\n",
    "testset = DinoDataset(root_dir = \"captures\", dataframe = test, transform = transformer)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bb9969-dcfd-4d0e-a558-a838aa9a1f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.virtualenvs\\CZ4042-NNDLProject\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "for i in range(len(images)):\n",
    "        ax = plt.subplot(2, 4, i + 1)\n",
    "        image = (images[i].permute(1,2,0)*255.0).cpu()\n",
    "        ax.set_title(labels[i].item(), fontsize=20)  # Setting the title of the subplot\n",
    "        ax.set_xticklabels([])   # Removing the x-axis labels\n",
    "        ax.set_yticklabels([])   # Removing the y-axis labels\n",
    "        plt.imshow(image)        # Plotting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa940a9-5373-4dae-9e12-77a2aa5d81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = torchvision.models.efficientnet_v2_s()\n",
    "model.classifier = torch.nn.Linear(in_features = 1280, out_features = 2)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b03a8d8-e08d-4d60-9b8d-baddb489d756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                      | 0/768 [00:00<?, ?it/s]C:\\Users\\user\\.virtualenvs\\CZ4042-NNDLProject\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 768/768 [1:03:54<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] | loss: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 768/768 [53:03<00:00,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] | loss: 0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 768/768 [55:07<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] | loss: 0.253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 768/768 [55:19<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] | loss: 0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 768/768 [8:25:13<00:00, 39.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] | loss: 0.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 768/768 [1:02:23<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] | loss: 0.210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                           | 2/768 [00:14<1:32:18,  7.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\.virtualenvs\\CZ4042-NNDLProject\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\CZ4042-NNDLProject\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 15  # number of training passes over the mini batches\n",
    "loss_container = [] # container to store the loss values after each epoch\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(trainloader, position=0, leave=True):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    loss_container.append(running_loss)\n",
    "\n",
    "    print(f'[{epoch + 1}] | loss: {running_loss / len(trainloader):.3f}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# plot the loss curve\n",
    "plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_container)\n",
    "\n",
    "# clean up the gpu memory \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ae6149-df12-48cb-aea3-802516e2d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'efficientnet_s.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3862a8b-291c-4280-abbe-1fc0ee5a2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = torchvision.models.efficientnet_v2_s()\n",
    "saved_model.classifier = torch.nn.Linear(in_features = 1280, out_features = 2)\n",
    "saved_model.load_state_dict(torch.load(PATH))\n",
    "saved_model = saved_model.to(device)\n",
    "saved_mode = saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bc1096d-524a-42cd-98d1-9c448daf3d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 192/192 [04:24<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n Accuracy of the network on the test images: 91 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "  for data in tqdm(testloader):\n",
    "    images,labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = saved_model(images)\n",
    "    predicted = torch.softmax(outputs,dim = 1).argmax(dim = 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'\\\\n Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6e6ca-54e6-4e73-90d7-f0f93eb5deb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
